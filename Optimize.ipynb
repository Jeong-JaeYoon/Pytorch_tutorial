{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 매개변수 최적화하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기본 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26422272it [00:22, 1184726.82it/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29696it [00:00, 117266.30it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4422656it [00:14, 302645.20it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6144it [00:00, 3082881.18it/s]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jj950\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "training_data = datasets.FashionMNIST(root='data', train=True, download=True, transform=ToTensor())\n",
    "\n",
    "test_data = datasets.FashionMNIST(root='data', download=True, train=False, transform=ToTensor())\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,10),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epoch = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최적화 단계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (x,y) in enumerate(dataloader):\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch*len(x)\n",
    "            print(f\"loss: {loss} [{current} / {size}]\")\n",
    "            \n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batch = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x,y in dataloader:\n",
    "            pred = model(x)\n",
    "            test_loss += loss_fn(pred,y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    test_loss /= num_batch\n",
    "    correct /= size\n",
    "    print(f'Test error: \\n Accuracy: {(100*correct):>.1f}%, Avg loss: {test_loss:>8f} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \n",
      "---------------------------\n",
      "loss: 2.29766583442688 [0 / 60000]\n",
      "loss: 2.2843403816223145 [6400 / 60000]\n",
      "loss: 2.2705724239349365 [12800 / 60000]\n",
      "loss: 2.276184320449829 [19200 / 60000]\n",
      "loss: 2.2399396896362305 [25600 / 60000]\n",
      "loss: 2.225576877593994 [32000 / 60000]\n",
      "loss: 2.240018367767334 [38400 / 60000]\n",
      "loss: 2.2093868255615234 [44800 / 60000]\n",
      "loss: 2.207151174545288 [51200 / 60000]\n",
      "loss: 2.1854772567749023 [57600 / 60000]\n",
      "Test error: \n",
      " Accuracy: 40.3%, Avg loss: 2.172835 \n",
      "\n",
      "Epoch 2 \n",
      "---------------------------\n",
      "loss: 2.1799182891845703 [0 / 60000]\n",
      "loss: 2.1633386611938477 [6400 / 60000]\n",
      "loss: 2.1186447143554688 [12800 / 60000]\n",
      "loss: 2.1428396701812744 [19200 / 60000]\n",
      "loss: 2.068453073501587 [25600 / 60000]\n",
      "loss: 2.02836537361145 [32000 / 60000]\n",
      "loss: 2.061814308166504 [38400 / 60000]\n",
      "loss: 1.9884768724441528 [44800 / 60000]\n",
      "loss: 1.9922658205032349 [51200 / 60000]\n",
      "loss: 1.9330787658691406 [57600 / 60000]\n",
      "Test error: \n",
      " Accuracy: 50.2%, Avg loss: 1.921228 \n",
      "\n",
      "Epoch 3 \n",
      "---------------------------\n",
      "loss: 1.947454571723938 [0 / 60000]\n",
      "loss: 1.9096990823745728 [6400 / 60000]\n",
      "loss: 1.8063682317733765 [12800 / 60000]\n",
      "loss: 1.8575891256332397 [19200 / 60000]\n",
      "loss: 1.7217308282852173 [25600 / 60000]\n",
      "loss: 1.6836875677108765 [32000 / 60000]\n",
      "loss: 1.7168049812316895 [38400 / 60000]\n",
      "loss: 1.6186752319335938 [44800 / 60000]\n",
      "loss: 1.6371883153915405 [51200 / 60000]\n",
      "loss: 1.5471560955047607 [57600 / 60000]\n",
      "Test error: \n",
      " Accuracy: 59.9%, Avg loss: 1.553562 \n",
      "\n",
      "Epoch 4 \n",
      "---------------------------\n",
      "loss: 1.613324522972107 [0 / 60000]\n",
      "loss: 1.5717262029647827 [6400 / 60000]\n",
      "loss: 1.433768630027771 [12800 / 60000]\n",
      "loss: 1.5085599422454834 [19200 / 60000]\n",
      "loss: 1.3739001750946045 [25600 / 60000]\n",
      "loss: 1.3770211935043335 [32000 / 60000]\n",
      "loss: 1.3931612968444824 [38400 / 60000]\n",
      "loss: 1.3228728771209717 [44800 / 60000]\n",
      "loss: 1.3476234674453735 [51200 / 60000]\n",
      "loss: 1.2563480138778687 [57600 / 60000]\n",
      "Test error: \n",
      " Accuracy: 63.1%, Avg loss: 1.277645 \n",
      "\n",
      "Epoch 5 \n",
      "---------------------------\n",
      "loss: 1.3535232543945312 [0 / 60000]\n",
      "loss: 1.3287413120269775 [6400 / 60000]\n",
      "loss: 1.1737172603607178 [12800 / 60000]\n",
      "loss: 1.276084303855896 [19200 / 60000]\n",
      "loss: 1.1451051235198975 [25600 / 60000]\n",
      "loss: 1.1754467487335205 [32000 / 60000]\n",
      "loss: 1.1921364068984985 [38400 / 60000]\n",
      "loss: 1.137892723083496 [44800 / 60000]\n",
      "loss: 1.1664239168167114 [51200 / 60000]\n",
      "loss: 1.0859633684158325 [57600 / 60000]\n",
      "Test error: \n",
      " Accuracy: 64.8%, Avg loss: 1.106079 \n",
      "\n",
      "Epoch 6 \n",
      "---------------------------\n",
      "loss: 1.1775529384613037 [0 / 60000]\n",
      "loss: 1.173750638961792 [6400 / 60000]\n",
      "loss: 1.0011783838272095 [12800 / 60000]\n",
      "loss: 1.1316909790039062 [19200 / 60000]\n",
      "loss: 1.0014028549194336 [25600 / 60000]\n",
      "loss: 1.038883090019226 [32000 / 60000]\n",
      "loss: 1.0673781633377075 [38400 / 60000]\n",
      "loss: 1.0187546014785767 [44800 / 60000]\n",
      "loss: 1.0479075908660889 [51200 / 60000]\n",
      "loss: 0.9785425066947937 [57600 / 60000]\n",
      "Test error: \n",
      " Accuracy: 66.0%, Avg loss: 0.995093 \n",
      "\n",
      "Epoch 7 \n",
      "---------------------------\n",
      "loss: 1.054193139076233 [0 / 60000]\n",
      "loss: 1.0721657276153564 [6400 / 60000]\n",
      "loss: 0.88210129737854 [12800 / 60000]\n",
      "loss: 1.035845398902893 [19200 / 60000]\n",
      "loss: 0.9098798632621765 [25600 / 60000]\n",
      "loss: 0.9414740800857544 [32000 / 60000]\n",
      "loss: 0.9855695366859436 [38400 / 60000]\n",
      "loss: 0.9400308728218079 [44800 / 60000]\n",
      "loss: 0.9654614925384521 [51200 / 60000]\n",
      "loss: 0.9062620997428894 [57600 / 60000]\n",
      "Test error: \n",
      " Accuracy: 67.1%, Avg loss: 0.919319 \n",
      "\n",
      "Epoch 8 \n",
      "---------------------------\n",
      "loss: 0.963478684425354 [0 / 60000]\n",
      "loss: 1.0012264251708984 [6400 / 60000]\n",
      "loss: 0.7965540289878845 [12800 / 60000]\n",
      "loss: 0.9681458473205566 [19200 / 60000]\n",
      "loss: 0.8483004570007324 [25600 / 60000]\n",
      "loss: 0.8691837787628174 [32000 / 60000]\n",
      "loss: 0.9281947612762451 [38400 / 60000]\n",
      "loss: 0.8863442540168762 [44800 / 60000]\n",
      "loss: 0.9055380821228027 [51200 / 60000]\n",
      "loss: 0.8543857932090759 [57600 / 60000]\n",
      "Test error: \n",
      " Accuracy: 68.4%, Avg loss: 0.864647 \n",
      "\n",
      "Epoch 9 \n",
      "---------------------------\n",
      "loss: 0.8937596082687378 [0 / 60000]\n",
      "loss: 0.9479154944419861 [6400 / 60000]\n",
      "loss: 0.7326377630233765 [12800 / 60000]\n",
      "loss: 0.9174929261207581 [19200 / 60000]\n",
      "loss: 0.8043127655982971 [25600 / 60000]\n",
      "loss: 0.814170777797699 [32000 / 60000]\n",
      "loss: 0.8850745558738708 [38400 / 60000]\n",
      "loss: 0.8482034802436829 [44800 / 60000]\n",
      "loss: 0.8604046106338501 [51200 / 60000]\n",
      "loss: 0.8150258660316467 [57600 / 60000]\n",
      "Test error: \n",
      " Accuracy: 69.6%, Avg loss: 0.823301 \n",
      "\n",
      "Epoch 10 \n",
      "---------------------------\n",
      "loss: 0.8382074236869812 [0 / 60000]\n",
      "loss: 0.9052051305770874 [6400 / 60000]\n",
      "loss: 0.6831225752830505 [12800 / 60000]\n",
      "loss: 0.8782221078872681 [19200 / 60000]\n",
      "loss: 0.771206796169281 [25600 / 60000]\n",
      "loss: 0.7715618014335632 [32000 / 60000]\n",
      "loss: 0.8505287170410156 [38400 / 60000]\n",
      "loss: 0.8198796510696411 [44800 / 60000]\n",
      "loss: 0.8251227736473083 [51200 / 60000]\n",
      "loss: 0.7837362289428711 [57600 / 60000]\n",
      "Test error: \n",
      " Accuracy: 70.7%, Avg loss: 0.790630 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch+1} \\n---------------------------')\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8b4d6a0c14dec02419a5baa7b05cba6417b25af0a78599953b93cb717ec482ae"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
